{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization Algorithms\n",
    "In this notebook you can find a collection of GD based optimization algorithm used for deep learning. The code is always accompanied by a explanatory youtube video which are linked here:\n",
    "- [Stochastic Gradient Descent](https://youtu.be/IH9kqpMORLM)\n",
    "- [Stochastic Gradient Descent + Momentum](https://youtu.be/7EuiXb6hFAM)\n",
    "- [Adagrad](https://youtu.be/EGt-UOIIdDk)\n",
    "- [RMSprop](https://youtu.be/nLCuzsQaAKE)\n",
    "- [AdaDelta](https://youtu.be/6gvh0IySNMs)\n",
    "- [Adam](https://youtu.be/6nqV58NA_Ew)\n",
    "- [Nesterov](https://youtu.be/6FrBXv9OcqE)\n",
    "- [Adamax](https://youtu.be/Uef4BofnVn0)\n",
    "- [Nadam](https://youtu.be/8nfd7gEDKCc)\n",
    "- QHM and QHAdam\n",
    "\n",
    "## Tests\n",
    "In order to demonstrate the algorithms capabilities to optimize a function we used these simple test setup:\n",
    "- learning various linear function of the form `f(x) = w0 + w1*x` with the squared error. This is a simple sanity check as the gradient are simple to calculate and the test data is also easy to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from numpy.random import permutation\n",
    "\n",
    "class Line():\n",
    "    \"\"\"\n",
    "        Linear Model with two weights w0 (intercept) and w1 (slope)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = [np.random.uniform(0,1,1) for _ in range(2)]\n",
    "        self.derivative_funcs = [self.dx_w0, self.dx_w1]\n",
    "        \n",
    "    def evaluate(self,x):\n",
    "        \"\"\"\n",
    "            evaluate: will evaluate the line yhate given x\n",
    "            x: a point in the plane\n",
    "\n",
    "            return the result of the function evalutation\n",
    "        \"\"\"\n",
    "        return self.weights[0] + self.weights[1]*x\n",
    "\n",
    "    def derivate(self, x, y):\n",
    "        \"\"\"\n",
    "            derivate: will calculate all partial derivatives and return them\n",
    "            input:\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "            \n",
    "            output:\n",
    "            partial_derivatives: an array of partial derivatives\n",
    "        \"\"\"\n",
    "        partial_derivatives = []\n",
    "        \n",
    "        yhat = self.evaluate(x)\n",
    "        partial_derivatives.append(self.dx_w0(x, y, yhat))\n",
    "        partial_derivatives.append(self.dx_w1(x, y, yhat))\n",
    "        \n",
    "        return partial_derivatives\n",
    "    \n",
    "    def dx_w0(self, x, y, yhat):\n",
    "        \"\"\"\n",
    "            dx_w0: partial derivative of the weight w0\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "            yhat: the current approximation of y given x and the weights\n",
    "\n",
    "            return the gradient at that point for this x and y for w0\n",
    "        \"\"\"\n",
    "        return 2*(yhat - y)\n",
    "    \n",
    "    def dx_w1(self, x, y, yhat):\n",
    "        \"\"\"\n",
    "            dx_w1: partial derivative of the weight w1 for a linear function\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "            yhat: the current approximation of y given x and the weights\n",
    "\n",
    "            return the gradient at that point for this x and y for w1\n",
    "        \"\"\"  \n",
    "        return 2*x*(yhat - y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"y = {self.weights[0]} + {self.weights[1]}*x\"\n",
    "        \n",
    "    \n",
    "#################### Helper functions ######################\n",
    "def stochastic_sample(xs, ys):\n",
    "    \"\"\"\n",
    "        stochastic_sample: sample with replacement one x and one y\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the randomly selected x and y point\n",
    "    \"\"\"\n",
    "    perm = permutation(len(xs))\n",
    "    x = xs[perm[0]]\n",
    "    y = ys[perm[0]]\n",
    "\n",
    "    return x, y\n",
    "    \n",
    "    \n",
    "def gradient(dx, evaluate, xs, ys):\n",
    "    \"\"\"\n",
    "        gradient: estimate mean gradient over all point for w1\n",
    "        evaluate: the evaulation function from the model\n",
    "        dx: partial derivative function used to evaluate the gradient\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the mean gradient all x and y for w1\n",
    "    \"\"\"         \n",
    "    N = len(ys)\n",
    "    \n",
    "    total = 0\n",
    "    for x,y in zip(xs,ys):\n",
    "        yhat = evaluate(x)\n",
    "        total = total + dx(x, y, yhat)\n",
    "    \n",
    "    gradient = total/N\n",
    "    return gradient\n",
    "\n",
    "################## Optimization Functions #####################\n",
    "\n",
    "def gd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        gd: will estimate the parameters w1 and w2 (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using gradient descent\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"    \n",
    "\n",
    "    for i in range(max_num_iteration):\n",
    "        # Updating the model parameters\n",
    "        model.weights = [weight - learning_rate*gradient(derivative_func, model.evaluate, xs, ys) for weight, derivative_func in zip(model.weights, model.derivative_funcs)]\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def sgd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        sgd: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"       \n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        model.weights = [weight - learning_rate*derivative for weight, derivative in zip(model.weights, model.derivate(x,y))]        \n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def sgd_momentum(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        sgd_momentum: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the gradient that we keep track as an array of 0 of the same size as the number of weights\n",
    "    gradients = [0 for _ in range(len(model.weights))]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "\n",
    "        # Calculate the new gradients\n",
    "        gradients = [decay_factor*g + learning_rate*derivative for g, derivative in zip(gradients, model.derivate(x,y))]\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        model.weights = [weight - g for weight, g in zip(model.weights, gradients)]\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "            \n",
    "def adagrad(model, xs, ys, learning_rate = 0.1, max_num_iteration = 1000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        adagrad: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    \n",
    "    # Here only the diagonal matter\n",
    "    num_param = len(model.weights)\n",
    "    G = [[0 for _ in range(num_param)] for _ in range(num_param)]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Update G and the model weights iteratively (Note: speed up could be gained from vectorized implementation)\n",
    "        for idx, gradient in enumerate(model.derivate(x, y)):\n",
    "            G[idx][idx] = G[idx][idx] + gradient**2\n",
    "            model.weights[idx] = model.weights[idx] - (learning_rate / np.sqrt(G[idx][idx] + eps)) * gradient\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def rmsprop(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 10000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        rmsprop: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        decay_factor: the parameter used in the running averaging\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    \n",
    "    # Running average\n",
    "    E = [0 for _ in range(len(model.weights))]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Update E and the model weights iteratively (Note: speed up could be gained from vectorized implementation)\n",
    "        for idx, gradient in enumerate(model.derivate(x, y)):    \n",
    "            E[idx] = decay_factor*E[idx] + (1 - decay_factor)*(gradient**2)\n",
    "            model.weights[idx] = model.weights[idx] - (learning_rate/np.sqrt(E[idx] + eps))*gradient\n",
    "\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "\n",
    "def adadelta(model, xs, ys, decay_factor = 0.9, max_num_iteration = 10000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        Adadelta: will estimate the parameters w0 and w1\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        decay_factor: the parameter used in the running averaging\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    \n",
    "    # Init Running Averages\n",
    "    num_param = len(model.weights)\n",
    "    E_g = [0 for _ in range(num_param)]\n",
    "    E_p = [0 for _ in range(num_param)]\n",
    "    delta_p = [0 for _ in range(num_param)]\n",
    "    \n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        for idx, gradient in enumerate(model.derivate(x, y)):\n",
    "            # Get the running average for the gradient\n",
    "            E_g[idx] = decay_factor*E_g[idx] + (1 - decay_factor)*(gradient**2)\n",
    "            \n",
    "            # Get the running average for the parameters\n",
    "            E_p[idx] = decay_factor*E_p[idx] + (1 - decay_factor)*(delta_p[idx]**2)\n",
    "            \n",
    "            # Calculate the gradient difference\n",
    "            delta_p[idx] = - np.sqrt(E_p[idx] + eps) / np.sqrt(E_g[idx] + eps) * gradient\n",
    "            \n",
    "            # update the model weight\n",
    "            model.weights[idx] = model.weights[idx] + delta_p[idx]\n",
    "            \n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "\n",
    "def adam(model, xs, ys, learning_rate = 0.1, b1 = 0.9, b2 = 0.999, epsilon = 0.00000001, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "        Adam: This is the adam optimizer that build upon adadelta and RMSProp\n",
    "        model: The model we want to optimize the parameter on\n",
    "        xs: the feature of my dataset\n",
    "        ys: the continous value (target)\n",
    "        learning_rate: the amount of learning we want to happen at each time step (default is 0.1 and will be updated by the optimizer)\n",
    "        b1: this is the first decaying average with proposed default value of 0.9 (deep learning purposes)\n",
    "        b2: this is the second decaying average with proposed default value of 0.999 (deep learning purposes)\n",
    "        epsilon: a variable for numerical stability during the division\n",
    "        max_iteration: the number of sgd round we want to do before stopping the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Variable Initialization\n",
    "    num_param = len(model.weights)\n",
    "    m = [0 for _ in range(num_param)] # two m for each parameter\n",
    "    v = [0 for _ in range(num_param)] # two v for each parameter\n",
    "    g = [0 for _ in range(num_param)] # two gradient\n",
    "    \n",
    "    for t in range(1,max_iteration):\n",
    "        \n",
    "        # Calculate the gradients \n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Get the partial derivatives\n",
    "        g = model.derivate(x, y)\n",
    "\n",
    "        # Update the m and v parameter\n",
    "        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]\n",
    "        v = [b2*v_i + (1 - b2)*(g_i**2) for v_i, g_i in zip(v, g)]\n",
    "\n",
    "        # Bias correction for m and v\n",
    "        m_cor = [m_i / (1 - (b1**t)) for m_i in m]\n",
    "        v_cor = [v_i / (1 - (b2**t)) for v_i in v]\n",
    "\n",
    "        # Update the parameter\n",
    "        model.weights = [weight - (learning_rate / (np.sqrt(v_cor_i) + epsilon))*m_cor_i for weight, v_cor_i, m_cor_i in zip(model.weights, v_cor, m_cor)]\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"Iteration {t}\")\n",
    "            print(model)\n",
    "    \n",
    "def nesterov(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        Nesterov: This is the nesterov accelerated gradient optimizer that build upon momentum\n",
    "        model: the model we want to optimize the parameter on (this is a line right now)\n",
    "        xs: the feature of my dataset\n",
    "        ys: the continous value (target)\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"\n",
    "    \n",
    "    # These are needed to keep track of the previous gradient\n",
    "    g = [0 for _ in range(len(model.weights))] \n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "\n",
    "        # Calculate the gradient for w0 by predicting where the ball will be (approximatively)\n",
    "        for idx, gradient in enumerate(model.derivate(x,y)):\n",
    "            \n",
    "            # Here we need to do a bit of gymnastic because of how the code is setup\n",
    "            # We need to save the parameters state, modify it, do the simulation and then reset the parameter state\n",
    "            # The update happen in the next section\n",
    "            prev_weight = model.weights[idx]\n",
    "            model.weights[idx] = decay_factor*gradient\n",
    "            g[idx] = decay_factor*g[idx] + learning_rate*gradient\n",
    "            model.weights[idx] = prev_weight\n",
    "            \n",
    "            # Update the model parameter\n",
    "            model.weights[idx] = model.weights[idx] - g[idx]\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "            \n",
    "def adamax(model, xs, ys, learning_rate = 0.1, b1 = 0.9, b2 = 0.999, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "        Adamax: This is the adamax optimizer that build upong adam with L_inf norm\n",
    "        model: The model we want to optimize the parameter on\n",
    "        xs: the feature of my dataset\n",
    "        ys: the continous value (target)\n",
    "        learning_rate: the amount of learning we want to happen at each time step (default is 0.1 and will be updated by the optimizer)\n",
    "        b1: this is the first decaying average with proposed default value of 0.9 (deep learning purposes)\n",
    "        b2: this is the second decaying average with proposed default value of 0.999 (deep learning purposes)\n",
    "        max_iteration: the number of sgd round we want to do before stopping the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Variable Initialization\n",
    "    num_param = len(model.weights)\n",
    "    m = [0 for _ in range(num_param)] # two m for each parameter\n",
    "    v = [0 for _ in range(num_param)] # two v for each parameter\n",
    "    g = [0 for _ in range(num_param)] # two gradient\n",
    "    \n",
    "    for t in range(1,max_iteration):\n",
    "        \n",
    "        # Calculate the gradients \n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Get the partial derivatives\n",
    "        g = model.derivate(x, y)\n",
    "\n",
    "        # Update the m and v parameter\n",
    "        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]\n",
    "        v = [np.maximum(b2*v_i, np.absolute(g_i)) for v_i, g_i in zip(v, g)]\n",
    "\n",
    "        # Bias correction for m only\n",
    "        m_cor = [m_i / (1 - (b1**t)) for m_i in m]\n",
    "\n",
    "        # Update the parameter\n",
    "        model.weights = [weight - (learning_rate / np.sqrt(v_i))*m_cor_i for weight, v_i, m_cor_i in zip(model.weights, v, m_cor)]\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"Iteration {t}\")\n",
    "            print(model)\n",
    "            \n",
    "def nadam(model, xs, ys, learning_rate = 0.1, b1 = 0.9, b2 = 0.999, epsilon = 0.00000001, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "        Nadam: This is the adam optimizer that build upon adadelta and RMSProp with Nesterov Accelerated Gradient sprinkled on top\n",
    "        model: The model we want to optimize the parameter on\n",
    "        xs: the feature of my dataset\n",
    "        ys: the continous value (target)\n",
    "        learning_rate: the amount of learning we want to happen at each time step (default is 0.1 and will be updated by the optimizer)\n",
    "        b1: this is the first decaying average with proposed default value of 0.9 (deep learning purposes)\n",
    "        b2: this is the second decaying average with proposed default value of 0.999 (deep learning purposes)\n",
    "        epsilon: a variable for numerical stability during the division\n",
    "        max_iteration: the number of sgd round we want to do before stopping the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Variable Initialization\n",
    "    num_param = len(model.weights)\n",
    "    m = [0 for _ in range(num_param)] # two m for each parameter\n",
    "    v = [0 for _ in range(num_param)] # two v for each parameter\n",
    "    g = [0 for _ in range(num_param)] # two gradient\n",
    "    \n",
    "    for t in range(1,max_iteration):\n",
    "        \n",
    "        # Calculate the gradients \n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Get the partial derivatives\n",
    "        g = model.derivate(x, y)\n",
    "\n",
    "        # Update the m and v parameter\n",
    "        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]\n",
    "        v = [b2*v_i + (1 - b2)*(g_i**2) for v_i, g_i in zip(v, g)]\n",
    "\n",
    "        # Bias correction for m and v\n",
    "        m_cor = [m_i / (1 - (b1**t)) for m_i in m]\n",
    "        v_cor = [v_i / (1 - (b2**t)) for v_i in v]\n",
    "\n",
    "        # nesterov look-ahead gradient\n",
    "        nag_g = [b1*m_cor_i + ((1-b1) * g_i)/(1 - b1**t) for m_cor_i, g_i in zip(m_cor, g)]\n",
    "        \n",
    "        # Update the parameter\n",
    "        model.weights = [weight - (learning_rate / (np.sqrt(v_cor_i) + epsilon))*nag_g_i for weight, v_cor_i, nag_g_i in zip(model.weights, v_cor, nag_g)]\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"Iteration {t}\")\n",
    "            print(model)\n",
    "\n",
    "\n",
    "def qhm(model, xs, ys, learning_rate = 0.01, decay_factor = 0.999, v = 0.7, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        QHM: will estimate the parameters w0 and w1 using a weighted average between SGD and Momentum.\n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        v: immediate discount factor ν\n",
    "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the gradient that we keep track as an array of 0 of the same size as the number of weights\n",
    "    gradients = [0 for _ in range(len(model.weights))]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "\n",
    "        # Calculate the new gradients\n",
    "        derivatives = model.derivate(x,y)\n",
    "        moment_buffer = [decay_factor*g + (1-decay_factor)*derivative for g, derivative in zip(gradients, derivatives)]\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        # TODO: Modify this for QHM\n",
    "        model.weights = [weight - learning_rate*((1-v)*derivative + v*g) for weight, g, derivative in zip(model.weights, moment_buffer, derivatives)]\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "\n",
    "def qhadam(model, xs, ys, learning_rate = 0.1, b1 = 0.95, b2 = 0.98, v1 = 0.8, v2 = 0.7, epsilon = 0.00000001, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "        QhAdam: This is the quasi hyperbolic modification of the adam optimizer (can recover lots of other form of Adam)\n",
    "        model: The model we want to optimize the parameter on\n",
    "        xs: the feature of my dataset\n",
    "        ys: the continous value (target)\n",
    "        learning_rate: the amount of learning we want to happen at each time step (default is 0.1 and will be updated by the optimizer)\n",
    "        b1: this is the first decaying average with proposed default value of 0.9 (deep learning purposes)\n",
    "        b2: this is the second decaying average with proposed default value of 0.999 (deep learning purposes)\n",
    "        v1: immediate discount factor for the first moment vector\n",
    "        v2: immediate discount factor for the second moment vector\n",
    "        epsilon: a variable for numerical stability during the division\n",
    "        max_iteration: the number of sgd round we want to do before stopping the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Variable Initialization\n",
    "    num_param = len(model.weights)\n",
    "    m = [0 for _ in range(num_param)] # two m for each parameter\n",
    "    v = [0 for _ in range(num_param)] # two v for each parameter\n",
    "    g = [0 for _ in range(num_param)] # two gradient\n",
    "    \n",
    "    for t in range(1,max_iteration):\n",
    "        \n",
    "        # Calculate the gradients \n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Get the partial derivatives\n",
    "        g = model.derivate(x, y)\n",
    "\n",
    "        # Update the m and v parameter\n",
    "        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]\n",
    "        v = [b2*v_i + (1 - b2)*(g_i**2) for v_i, g_i in zip(v, g)]\n",
    "\n",
    "        # Bias correction for m and v\n",
    "        m_cor = [m_i / (1 - (b1**t)) for m_i in m]\n",
    "        v_cor = [v_i / (1 - (b2**t)) for v_i in v]\n",
    "\n",
    "        # Update the parameter\n",
    "        # TODO: Modify this for QHAdam\n",
    "        model.weights = [weight - learning_rate*(m_cor_i / (np.sqrt(v_cor_i) + epsilon)) for weight, v_cor_i, m_cor_i in zip(model.weights, v_cor, m_cor)]\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"Iteration {t}\")\n",
    "            print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: intercept = 0 and slope = 1\n",
      "Nadam\n",
      "Iteration 100\n",
      "y = [0.00129164] + [0.9997128]*x\n",
      "Iteration 200\n",
      "y = [3.589196e-06] + [0.9999991]*x\n",
      "Iteration 300\n",
      "y = [1.33508926e-08] + [1.]*x\n",
      "Iteration 400\n",
      "y = [-1.53784677e-12] + [1.]*x\n",
      "Iteration 500\n",
      "y = [3.89991677e-11] + [1.]*x\n",
      "Iteration 600\n",
      "y = [2.33624382e-07] + [0.99999989]*x\n",
      "Iteration 700\n",
      "y = [0.02489509] + [0.98218936]*x\n",
      "Iteration 800\n",
      "y = [-0.00164908] + [0.99994338]*x\n",
      "Iteration 900\n",
      "y = [1.89338839e-07] + [0.99999995]*x\n",
      "y = [-2.43266752e-09] + [1.]*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 1\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [1,2,3,4,5,6,7]\n",
    "\n",
    "print(\"Target: intercept = 0 and slope = 1\")\n",
    "\n",
    "'''\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# RMSprop\n",
    "model = Line()\n",
    "print(\"RMSprop\")\n",
    "rmsprop(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adadelta\n",
    "model = Line()\n",
    "print(\"Adadelta\")\n",
    "adadelta(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adam\n",
    "model = Line()\n",
    "print(\"Adam\")\n",
    "adam(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Nesterov Accelerated Gradient\n",
    "model = Line()\n",
    "print(\"Nesterov Accelerated Gradient\")\n",
    "nesterov(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adamax\n",
    "model = Line()\n",
    "print(\"Adamax\")\n",
    "adamax(model, xs, ys)\n",
    "print(model)\n",
    "'''\n",
    "\n",
    "# Nadam\n",
    "model = Line()\n",
    "print(\"Nadam\")\n",
    "nadam(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: intercept = 0 and slope = 2\n",
      "Nadam\n",
      "Iteration 100\n",
      "y = [0.07145738] + [1.97704364]*x\n",
      "Iteration 200\n",
      "y = [-0.00055948] + [2.00004661]*x\n",
      "Iteration 300\n",
      "y = [2.30188812e-06] + [1.99999918]*x\n",
      "Iteration 400\n",
      "y = [-2.76462778e-10] + [2.]*x\n",
      "Iteration 500\n",
      "y = [2.34822724e-11] + [2.]*x\n",
      "Iteration 600\n",
      "y = [3.62729038e-13] + [2.]*x\n",
      "Iteration 700\n",
      "y = [1.28183436e-15] + [2.]*x\n",
      "Iteration 800\n",
      "y = [1.75441222e-16] + [2.]*x\n",
      "Iteration 900\n",
      "y = [1.75787821e-16] + [2.]*x\n",
      "y = [1.75787831e-16] + [2.]*x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here we have a simple line with intercept = 0 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [2,4,6,8,10,12,14]\n",
    "\n",
    "print(\"Target: intercept = 0 and slope = 2\")\n",
    "\n",
    "'''\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# RMSprop\n",
    "model = Line()\n",
    "print(\"RMSprop\")\n",
    "rmsprop(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adadelta\n",
    "model = Line()\n",
    "print(\"Adadelta\")\n",
    "adadelta(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adam\n",
    "model = Line()\n",
    "print(\"Adam\")\n",
    "adam(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Nesterov Accelerated Gradient\n",
    "model = Line()\n",
    "print(\"Nesterov Accelerated Gradient\")\n",
    "nesterov(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adamax\n",
    "model = Line()\n",
    "print(\"Adamax\")\n",
    "adamax(model, xs, ys)\n",
    "print(model)\n",
    "'''\n",
    "\n",
    "# Nadam\n",
    "model = Line()\n",
    "print(\"Nadam\")\n",
    "nadam(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: intercept = 1 and slope = 2\n",
      "Nadam\n",
      "Iteration 100\n",
      "y = [1.12937701] + [1.96998862]*x\n",
      "Iteration 200\n",
      "y = [1.01118693] + [1.99734387]*x\n",
      "Iteration 300\n",
      "y = [1.00018734] + [1.99994987]*x\n",
      "Iteration 400\n",
      "y = [0.99999842] + [2.00000016]*x\n",
      "Iteration 500\n",
      "y = [1.00000001] + [2.]*x\n",
      "Iteration 600\n",
      "y = [1.] + [2.]*x\n",
      "Iteration 700\n",
      "y = [1.] + [2.]*x\n",
      "Iteration 800\n",
      "y = [1.] + [2.]*x\n",
      "Iteration 900\n",
      "y = [1.] + [2.]*x\n",
      "y = [1.] + [2.]*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 1 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [3,5,7,9,11,13,15]\n",
    "\n",
    "print(\"Target: intercept = 1 and slope = 2\")\n",
    "\n",
    "'''\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# RMSprop\n",
    "model = Line()\n",
    "print(\"RMSprop\")\n",
    "rmsprop(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adadelta\n",
    "model = Line()\n",
    "print(\"Adadelta\")\n",
    "adadelta(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adam\n",
    "model = Line()\n",
    "print(\"Adam\")\n",
    "adam(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Nesterov Accelerated Gradient\n",
    "model = Line()\n",
    "print(\"Nesterov Accelerated Gradient\")\n",
    "nesterov(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adamax\n",
    "model = Line()\n",
    "print(\"Adamax\")\n",
    "adamax(model, xs, ys)\n",
    "print(model)\n",
    "'''\n",
    "\n",
    "# Nadam\n",
    "model = Line()\n",
    "print(\"Nadam\")\n",
    "nadam(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
